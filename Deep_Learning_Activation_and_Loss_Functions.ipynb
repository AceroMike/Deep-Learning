{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning: Activation and Loss Functions.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPm8io9WgVxCeon5mUOkDwU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AceroMike/Deep-Learning/blob/main/Deep_Learning_Activation_and_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7c8SDbLmhTz"
      },
      "source": [
        "# Imports\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "\r\n",
        "# Data\r\n",
        "from tensorflow.keras.datasets import mnist\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVUbo_2vmA8l"
      },
      "source": [
        "In the previous notebook, I looked at building some simple ANN models. However, I did not compare different loss and activation functions. In this notebook I will again work with the MNIST digits dataset and compare ANN models with different specifications. Let's learn about loss functions and activation functions. First, let's download the dataset:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m8DrEH_l9PK",
        "outputId": "acb8d701-ed5d-4e57-997e-f893e71f2e0d"
      },
      "source": [
        "# Loading and Preprocessing the Data\r\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "input_dim = 784  # 28*28\r\n",
        "output_dim = nb_classes = 10\r\n",
        "batch_size = 128\r\n",
        "nb_epoch = 20\r\n",
        "\r\n",
        "# We will be working on a sample\r\n",
        "X_train = X_train.reshape(60000, input_dim)\r\n",
        "X_test = X_test.reshape(10000, input_dim)\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "\r\n",
        "# This normalizes the data\r\n",
        "X_train /= 255\r\n",
        "X_test /= 255"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slw-6567nAMu"
      },
      "source": [
        "Now we want to one hot encode our target variable. Right now, our target variable tells us the actual number. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp37avvmnIgz",
        "outputId": "ec67a4e2-c631-492f-8d55-f1d32460e483"
      },
      "source": [
        "y_train[0] #First number is 5"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvGz-hKTm6Y3",
        "outputId": "0fac7559-32d1-4934-89f5-2d88d83c0776"
      },
      "source": [
        "Y_train = to_categorical(y_train, nb_classes)\r\n",
        "Y_test = to_categorical(y_test, nb_classes)\r\n",
        "Y_train[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtwG9MKFnQR1"
      },
      "source": [
        "If you look at the array we now see a 1 in the 6th spot, which corresponds to the number 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "WIspRg5hnMUE",
        "outputId": "6818530a-8787-4c20-cba5-8866271eb311"
      },
      "source": [
        "# Visualization\r\n",
        "plt.figure(figsize=(7,7))\r\n",
        "plt.imshow(X_train[0].reshape(28,28), cmap=\"gray\")\r\n",
        "plt.title(\"Label of the image: {}\".format(y_train[0]))\r\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQm0lrotoHPi"
      },
      "source": [
        "Now we can build some ANNs. The purpose of this notebook is to compare the results of models that use different activation and loss functions. So, that begs the questions, **What are activation and loss functions?**\r\n",
        "\r\n",
        "**Activation Functions**\r\n",
        "\r\n",
        "Activation functions are important because they introduce non-linearities in our data. In fact, without activation functions we would just be stacking linear functions which would not be meaningfull. Why? Because then the stacking is redundant because we can expressed the stacked layers as a single linear combination of the others and so we could have just expressed as a single layer. Activation functions are crucial in deep learning\r\n",
        "\r\n",
        "**Loss Functions**\r\n",
        "\r\n",
        "The loss function we choose will guide the model in estimating the parameters because the model will try to minimize the loss/cost. Some loss functions are better than others depending on the task but they all tell us the same. They allow us to measure how well our model is performing. \r\n",
        "\r\n",
        "For both Activation and Loss functions, there are separate functions to use if we have a classification or a regression task. In this notebook we will only cover some of the functions used for classification. \r\n",
        "\r\n",
        "Now let's build some models! As usual, when building models, I want to take a *ceteris paribus* approach. Which is fancy econ for saying that I only want to one thing at a time\r\n",
        "\r\n",
        "**ANN models**\r\n",
        "\r\n",
        "For the first 3 models we will build three-layer ANN model with 128, 64, and 10 neurons in the layers. We will choose a different activation function for each:\r\n",
        "sigmoid, tanh, and ReLU. We will discuss each briefly as we model. Now to define a function that wil build the model without having to type it out 3 times. KISS\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdwKYOGDndpQ"
      },
      "source": [
        "def build_model(activation='sigmoid', loss='categorical_crossentropy', optimizer='sgd'):\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Dense(128, input_shape=(784,), activation=activation))\r\n",
        "  model.add(Dense(64, input_shape=(784,), activation=activation))\r\n",
        "  model.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spO8TU8Q1NRq"
      },
      "source": [
        "**Sigmoid model**\r\n",
        "\r\n",
        "Mathematically, the sigmoid function is defined as follows: \r\n",
        "\r\n",
        "$$\\sigma(x)=1+\\dfrac{1}{e^{-x}}$$\r\n",
        "\r\n",
        "While being a popular activation function, it does have it's drawbacks:\r\n",
        "\r\n",
        "* **It's nonzero centered.** It results in zigzags during optimization.\r\n",
        "* **It results in vanishing gradient or saturation.** For high values of input values, the learning becomes very, very slow because the gradient (derivatives) becomes very close to zero! You'll learn about gradients in the next checkpoint.\r\n",
        "* **Because of the vanishing gradient, a careful initialization of the weights of the network is important.** But finding good enough values for initializing the parameters isn't always trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKbfMDs_1FdX"
      },
      "source": [
        "model = build_model(activation='sigmoid')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjkHe3xY1PQY",
        "outputId": "c91b9dee-8dba-4eac-e76a-f516f980fe96"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 109,386\n",
            "Trainable params: 109,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0nvDiNq1SOR",
        "outputId": "d874ccea-ec8e-49b6-8758-cac0285c8b40"
      },
      "source": [
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 2.3195 - accuracy: 0.1358 - val_loss: 2.2472 - val_accuracy: 0.3187\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 2.2337 - accuracy: 0.3581 - val_loss: 2.1885 - val_accuracy: 0.4844\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 2.1713 - accuracy: 0.4908 - val_loss: 2.1101 - val_accuracy: 0.5679\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 2.0857 - accuracy: 0.5655 - val_loss: 1.9974 - val_accuracy: 0.6002\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.9657 - accuracy: 0.5976 - val_loss: 1.8402 - val_accuracy: 0.6318\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.7974 - accuracy: 0.6304 - val_loss: 1.6450 - val_accuracy: 0.6519\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.6007 - accuracy: 0.6572 - val_loss: 1.4417 - val_accuracy: 0.6889\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.4054 - accuracy: 0.6880 - val_loss: 1.2629 - val_accuracy: 0.7145\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.2333 - accuracy: 0.7223 - val_loss: 1.1190 - val_accuracy: 0.7487\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.1015 - accuracy: 0.7531 - val_loss: 1.0057 - val_accuracy: 0.7716\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9899 - accuracy: 0.7718 - val_loss: 0.9159 - val_accuracy: 0.7914\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9028 - accuracy: 0.7916 - val_loss: 0.8425 - val_accuracy: 0.8042\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.8376 - accuracy: 0.8023 - val_loss: 0.7816 - val_accuracy: 0.8156\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.7819 - accuracy: 0.8149 - val_loss: 0.7302 - val_accuracy: 0.8283\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.7319 - accuracy: 0.8239 - val_loss: 0.6866 - val_accuracy: 0.8359\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6877 - accuracy: 0.8331 - val_loss: 0.6489 - val_accuracy: 0.8431\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6523 - accuracy: 0.8406 - val_loss: 0.6159 - val_accuracy: 0.8509\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6243 - accuracy: 0.8461 - val_loss: 0.5873 - val_accuracy: 0.8557\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5922 - accuracy: 0.8528 - val_loss: 0.5624 - val_accuracy: 0.8612\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5701 - accuracy: 0.8577 - val_loss: 0.5406 - val_accuracy: 0.8638\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd03253ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB9DMrUV5LGX"
      },
      "source": [
        "**Hyperbolic Tangent**\r\n",
        "\r\n",
        "*Hyperbolic tangent* (tanh) is actually a scalled version of the sigmoid function:\r\n",
        "\r\n",
        "$$tanh(x)=2\\sigma(2x)-1$$\r\n",
        "\r\n",
        "Unlike sigmoid, it's zero centered. However, like sigmoid, it also saturates and results in a vanishing gradient problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpU9xyPh5KrG",
        "outputId": "099c4552-1593-41bb-da15-3a874211fd7a"
      },
      "source": [
        "model = build_model(activation='tanh')\r\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=20, verbose=1)\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 1.4323 - accuracy: 0.6241 - val_loss: 0.5782 - val_accuracy: 0.8671\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5508 - accuracy: 0.8660 - val_loss: 0.4292 - val_accuracy: 0.8907\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4274 - accuracy: 0.8885 - val_loss: 0.3737 - val_accuracy: 0.9009\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3825 - accuracy: 0.8953 - val_loss: 0.3414 - val_accuracy: 0.9070\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3481 - accuracy: 0.9032 - val_loss: 0.3201 - val_accuracy: 0.9124\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3298 - accuracy: 0.9082 - val_loss: 0.3048 - val_accuracy: 0.9150\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3125 - accuracy: 0.9120 - val_loss: 0.2935 - val_accuracy: 0.9194\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2980 - accuracy: 0.9153 - val_loss: 0.2829 - val_accuracy: 0.9214\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2843 - accuracy: 0.9197 - val_loss: 0.2743 - val_accuracy: 0.9221\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2818 - accuracy: 0.9200 - val_loss: 0.2665 - val_accuracy: 0.9246\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2726 - accuracy: 0.9221 - val_loss: 0.2593 - val_accuracy: 0.9267\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2664 - accuracy: 0.9231 - val_loss: 0.2531 - val_accuracy: 0.9284\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2535 - accuracy: 0.9277 - val_loss: 0.2470 - val_accuracy: 0.9301\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2522 - accuracy: 0.9278 - val_loss: 0.2416 - val_accuracy: 0.9312\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2424 - accuracy: 0.9316 - val_loss: 0.2356 - val_accuracy: 0.9325\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2329 - accuracy: 0.9320 - val_loss: 0.2310 - val_accuracy: 0.9342\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2335 - accuracy: 0.9318 - val_loss: 0.2260 - val_accuracy: 0.9353\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2265 - accuracy: 0.9353 - val_loss: 0.2215 - val_accuracy: 0.9357\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2218 - accuracy: 0.9363 - val_loss: 0.2171 - val_accuracy: 0.9375\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2135 - accuracy: 0.9395 - val_loss: 0.2119 - val_accuracy: 0.9389\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd02858150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aEYyt6k6l8B"
      },
      "source": [
        "**Rectified Linear Units (ReLU)**\r\n",
        "\r\n",
        "*Rectified linear unit* (ReLU) is probably the most common activation function in deep-learning literature. Some properties of ReLU are as follows:\r\n",
        "\r\n",
        "* It cuts off values below zero.\r\n",
        "* It's nonsaturating.\r\n",
        "* It enables models to converge faster than sigmoid and tanh.\r\n",
        "* It's easy to implement.\r\n",
        "* However, using ReLU in the networks may cause some neurons to irreversibly die! If you set the learning rate too high, as many as 40% of the neurons can die."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sDjsnh458fb",
        "outputId": "e034bef1-0339-4bd8-fed2-e7568d998a9a"
      },
      "source": [
        "model = build_model(activation='relu')\r\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.7723 - accuracy: 0.4883 - val_loss: 0.6090 - val_accuracy: 0.8511\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5624 - accuracy: 0.8547 - val_loss: 0.4110 - val_accuracy: 0.8893\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4097 - accuracy: 0.8873 - val_loss: 0.3494 - val_accuracy: 0.9006\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3584 - accuracy: 0.8997 - val_loss: 0.3154 - val_accuracy: 0.9101\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3251 - accuracy: 0.9087 - val_loss: 0.2914 - val_accuracy: 0.9164\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3000 - accuracy: 0.9135 - val_loss: 0.2746 - val_accuracy: 0.9211\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2801 - accuracy: 0.9203 - val_loss: 0.2605 - val_accuracy: 0.9244\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2647 - accuracy: 0.9236 - val_loss: 0.2476 - val_accuracy: 0.9309\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2511 - accuracy: 0.9271 - val_loss: 0.2369 - val_accuracy: 0.9316\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2344 - accuracy: 0.9335 - val_loss: 0.2272 - val_accuracy: 0.9351\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2315 - accuracy: 0.9326 - val_loss: 0.2185 - val_accuracy: 0.9376\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2159 - accuracy: 0.9375 - val_loss: 0.2129 - val_accuracy: 0.9384\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2076 - accuracy: 0.9399 - val_loss: 0.2044 - val_accuracy: 0.9410\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.2033 - accuracy: 0.9409 - val_loss: 0.1973 - val_accuracy: 0.9431\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1944 - accuracy: 0.9442 - val_loss: 0.1908 - val_accuracy: 0.9440\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1893 - accuracy: 0.9456 - val_loss: 0.1862 - val_accuracy: 0.9451\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1799 - accuracy: 0.9482 - val_loss: 0.1814 - val_accuracy: 0.9460\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1802 - accuracy: 0.9480 - val_loss: 0.1757 - val_accuracy: 0.9477\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1766 - accuracy: 0.9494 - val_loss: 0.1718 - val_accuracy: 0.9507\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.1690 - accuracy: 0.9512 - val_loss: 0.1693 - val_accuracy: 0.9501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd006b4a50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6AN8ZdN7HJR"
      },
      "source": [
        "**Which model is best?**\r\n",
        "\r\n",
        "All models seem to work particularly well but the models that used Hyperbolic Tangent and Rectified Linear Units activation functions performed better. The best model was the Rectified Linear Units. With the highest accuracy and lowest loss. \r\n",
        "\r\n",
        "Now we want to change the loss function. We will change it to the Hinge loss function. Holding everything else constant. \r\n",
        "\r\n",
        "**Hinge Loss Function**\r\n",
        "\r\n",
        "The correct category's score should be greater than the sum of the scores of all incorrect categories by some safety margin. (Usually, the safety margin is set to `1`.) Hence, *hinge loss* is used for **maximum-margin classification**. Most notably, hinge loss is used for support vector machines. Defined as follows:\r\n",
        "\r\n",
        "$$\\sum_{j \\neq y_i}max(0, s_j-s_{y_i}+1)$$\r\n",
        "\r\n",
        "Here, $s_j$ is the maximum of predicted decisions for all other labels, and $s_{y_i}$ is the predicted decision for the true label. \r\n",
        "\r\n",
        "**Sigmoid**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfN7SQxz60NY",
        "outputId": "cb65b347-230b-44a1-e653-4d2e1393275c"
      },
      "source": [
        "model = build_model(activation='sigmoid', loss='hinge')\r\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 1.0795 - accuracy: 0.1518 - val_loss: 1.0794 - val_accuracy: 0.1723\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0795 - accuracy: 0.1749 - val_loss: 1.0794 - val_accuracy: 0.1657\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0794 - accuracy: 0.1589 - val_loss: 1.0793 - val_accuracy: 0.1442\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0794 - accuracy: 0.1387 - val_loss: 1.0793 - val_accuracy: 0.1290\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0793 - accuracy: 0.1226 - val_loss: 1.0792 - val_accuracy: 0.1177\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0792 - accuracy: 0.1163 - val_loss: 1.0792 - val_accuracy: 0.1145\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0792 - accuracy: 0.1115 - val_loss: 1.0791 - val_accuracy: 0.1136\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0792 - accuracy: 0.1111 - val_loss: 1.0791 - val_accuracy: 0.1135\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0791 - accuracy: 0.1121 - val_loss: 1.0790 - val_accuracy: 0.1135\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0791 - accuracy: 0.1135 - val_loss: 1.0790 - val_accuracy: 0.1135\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0791 - accuracy: 0.1116 - val_loss: 1.0789 - val_accuracy: 0.1135\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0789 - accuracy: 0.1134 - val_loss: 1.0788 - val_accuracy: 0.1135\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0789 - accuracy: 0.1107 - val_loss: 1.0788 - val_accuracy: 0.1135\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0788 - accuracy: 0.1130 - val_loss: 1.0787 - val_accuracy: 0.1135\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0788 - accuracy: 0.1112 - val_loss: 1.0786 - val_accuracy: 0.1135\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0786 - accuracy: 0.1139 - val_loss: 1.0786 - val_accuracy: 0.1135\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0785 - accuracy: 0.1154 - val_loss: 1.0785 - val_accuracy: 0.1135\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0785 - accuracy: 0.1134 - val_loss: 1.0784 - val_accuracy: 0.1135\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0784 - accuracy: 0.1121 - val_loss: 1.0783 - val_accuracy: 0.1135\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0784 - accuracy: 0.1112 - val_loss: 1.0782 - val_accuracy: 0.1135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd005e38d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjCvlpy1-zyJ"
      },
      "source": [
        "**Hyperbolic Tangent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0f2Xe4p-xQM",
        "outputId": "cb044a39-7873-4cbc-f3f5-4062c06fa087"
      },
      "source": [
        "model = build_model(activation='tanh', loss='hinge')\r\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 1.0785 - accuracy: 0.1577 - val_loss: 1.0752 - val_accuracy: 0.2210\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0741 - accuracy: 0.2414 - val_loss: 1.0696 - val_accuracy: 0.3138\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0681 - accuracy: 0.3311 - val_loss: 1.0618 - val_accuracy: 0.4156\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0597 - accuracy: 0.4415 - val_loss: 1.0511 - val_accuracy: 0.5165\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0481 - accuracy: 0.5151 - val_loss: 1.0380 - val_accuracy: 0.5257\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0358 - accuracy: 0.5216 - val_loss: 1.0275 - val_accuracy: 0.5377\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0265 - accuracy: 0.5341 - val_loss: 1.0194 - val_accuracy: 0.5519\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0187 - accuracy: 0.5466 - val_loss: 1.0131 - val_accuracy: 0.5593\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0126 - accuracy: 0.5547 - val_loss: 1.0081 - val_accuracy: 0.5650\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0080 - accuracy: 0.5599 - val_loss: 1.0040 - val_accuracy: 0.5704\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0044 - accuracy: 0.5623 - val_loss: 1.0004 - val_accuracy: 0.5807\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0010 - accuracy: 0.5763 - val_loss: 0.9964 - val_accuracy: 0.6109\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9963 - accuracy: 0.6118 - val_loss: 0.9919 - val_accuracy: 0.6468\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9921 - accuracy: 0.6428 - val_loss: 0.9882 - val_accuracy: 0.6606\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9884 - accuracy: 0.6575 - val_loss: 0.9853 - val_accuracy: 0.6672\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9856 - accuracy: 0.6626 - val_loss: 0.9828 - val_accuracy: 0.6720\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9833 - accuracy: 0.6695 - val_loss: 0.9806 - val_accuracy: 0.6781\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9810 - accuracy: 0.6763 - val_loss: 0.9787 - val_accuracy: 0.6843\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9788 - accuracy: 0.6824 - val_loss: 0.9770 - val_accuracy: 0.6881\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9779 - accuracy: 0.6839 - val_loss: 0.9754 - val_accuracy: 0.6915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efcff463950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bie5_Ea_Gna"
      },
      "source": [
        "**ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f15lXrPZ_JR-",
        "outputId": "75f9ea2f-f1c3-4fb4-fee7-85ab5fa04255"
      },
      "source": [
        "model = build_model(activation='relu', loss='hinge')\r\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 3s 4ms/step - loss: 1.0790 - accuracy: 0.1048 - val_loss: 1.0781 - val_accuracy: 0.1151\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0779 - accuracy: 0.1149 - val_loss: 1.0766 - val_accuracy: 0.1204\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0763 - accuracy: 0.1211 - val_loss: 1.0745 - val_accuracy: 0.1232\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0743 - accuracy: 0.1244 - val_loss: 1.0721 - val_accuracy: 0.1295\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0719 - accuracy: 0.1319 - val_loss: 1.0696 - val_accuracy: 0.1457\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0695 - accuracy: 0.1493 - val_loss: 1.0670 - val_accuracy: 0.1749\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0667 - accuracy: 0.1869 - val_loss: 1.0637 - val_accuracy: 0.2460\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0629 - accuracy: 0.2671 - val_loss: 1.0593 - val_accuracy: 0.3177\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0588 - accuracy: 0.3324 - val_loss: 1.0543 - val_accuracy: 0.3649\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0535 - accuracy: 0.3786 - val_loss: 1.0488 - val_accuracy: 0.3915\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0479 - accuracy: 0.3961 - val_loss: 1.0426 - val_accuracy: 0.4034\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0417 - accuracy: 0.4085 - val_loss: 1.0363 - val_accuracy: 0.4136\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0353 - accuracy: 0.4190 - val_loss: 1.0304 - val_accuracy: 0.4349\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0295 - accuracy: 0.4456 - val_loss: 1.0244 - val_accuracy: 0.4830\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0231 - accuracy: 0.4976 - val_loss: 1.0174 - val_accuracy: 0.5316\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0163 - accuracy: 0.5336 - val_loss: 1.0112 - val_accuracy: 0.5448\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0105 - accuracy: 0.5433 - val_loss: 1.0063 - val_accuracy: 0.5528\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0052 - accuracy: 0.5562 - val_loss: 1.0019 - val_accuracy: 0.5693\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0013 - accuracy: 0.5806 - val_loss: 0.9967 - val_accuracy: 0.6193\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.9961 - accuracy: 0.6227 - val_loss: 0.9920 - val_accuracy: 0.6372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efcfc2be410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P624NeUI_Y7e"
      },
      "source": [
        "As we can see, using the appropriate loss function is important and for this particular task, it seems that we want to stick with categorical cross entropy. So what is categorical cross entropy and why did it work out well?\r\n",
        "\r\n",
        "**Cross-entropy loss**\r\n",
        "\r\n",
        "*Cross-entropy loss* is the most common loss function for classification problems. Cross-entropy loss increases as the predicted probability diverges from the actual label. THis makes sense in our case since we are labeling integers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9UE_qpb_LB1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}